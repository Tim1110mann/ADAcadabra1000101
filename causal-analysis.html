<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Causal Analysis</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <header>
        <div class="container">
            <nav>
               <div class="link-1">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/about.html">About</a></li>
                    </ul>
                </div>
                <div class = "link-2">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/">Home</a></li>
                    </ul>
                </div>
                <div class = "link-3">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/preprocessing.html">Pre-Processing</a></li>
                    </ul>
                </div>
                <div class = "link-5">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/causal-analysis.html">Causal Analysis</a></li>
                    </ul>
                </div>
                <div class = "link-4">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/unsupervised-learning.html">Unsupervised Learning</a></li>
                    </ul>
                </div>
                <div class = "link-6">
                    <ul>
                        <li><a href="https://tim1110mann.github.io/ADAcadabra1000101/conclusion.html">Conclusion</a></li>
                    </ul>
                </div>
            </nav>
        </div>
    </header>
    <h1>Causal Analysis</h1>
    <div class="body-content">
        <p>
            Welcome to Causal Analysis!
        </p>
        <p>
            I hope the <a href="https://tim1110mann.github.io/ADAcadabra1000101/unsupervised-learning.html">Unsupervised Learning</a> narrator didn't say anything too mean to you while you were over there.
            Oh, you came to me first? That's awfully kind of you :).
            Anyway, enough blabbing around, let's dive straight in!
        </p>
        <br>
        <p>
            From our <a href="https://tim1110mann.github.io/ADAcadabra1000101/preprocessing.html">pre-processing</a>, we defined the key terms of our main question, the<b>features</b> and the <b>character tropes</b>.
            In this chapter, we are opting for a more causal approach.
            In other words, given an input <code>X</code>, we can find the output <code>y</code> via a function <code>f(X)</code>, with <code>f(X)</code> being the main mystery.
        </p>
        <p>
            We can apply this approach to our case and define our input <code>X</code> as the set of features we defined a chapter ago and our output <code>y</code> as our 16 unique character tropes.
            We would like to "learn" the function <code>f</code> that links the two such that <code>f(X) = y</code>.
            Who knows? We might even be able to have fun after having found this function, like figure out what kind of trope I would play!
        </p>
        <br>
        <p>
            As we have a discrete y, each character trope represents a separate and independent category, we have to go with classification.
            We will go through various methods to find our <code>f(X)</code>.
            Let's start with Decision Trees!
        </p>
        <h2>A Tree of Decisions</h2>
        <p>
            We had the choice between kNN or Decisions Trees as our preferred algorithm of classification. 
            We played around with both, testing them on our features and after seeing our results we opted for a decision tree. Yay!
            Now that we planted our tree, what's so special about it?
        </p>
        <p>
            To answer briefly, the <b>F1-score</b>... amongst other things but I'll save that for later.
        </p>
        <p>
            The F1-score is a measure of predictive performance. 
            And so in our case, the higher the F1-score, the better our classification algorithm can find a pattern of features that best describe our tropes.
            We compute the F1-score for each of our 16 character tropes and get the following table:
            <figure>
                <div id="table-container">
                    <table border="1">
                        <thead>
                            <tr>
                                <th>Tropes</th>
                                <th>F1-Score</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><code>charismatic_charmer</code></td>
                                <td>0.36</td>
                            </tr>
                            <tr>
                                <td><code>crazy_fighter</code></td>
                                <td>0.13</td>
                            </tr>
                            <tr>
                                <td><code>crazy_jealous_guy</code></td>
                                <td>0.10</td>
                            </tr>
                            <tr>
                                <td><code>dumb_and_clumsy</code></td>
                                <td>0.26</td>
                            </tr>
                            <tr>
                                <td><code>emotional_damage</code></td>
                                <td>0.24</td>
                            </tr>
                            <tr>
                                <td><code>evil_character</code></td>
                                <td>0.05</td>
                            </tr>
                            <tr>
                                <td><code>jock</code></td>
                                <td>0.26</td>
                            </tr>
                            <tr>
                                <td><code>laidback_freebird</code></td>
                                <td>0.50</td>
                            </tr>
                            <tr>
                                <td><code>loser</code></td>
                                <td>0.09</td>
                            </tr>
                            <tr>
                                <td><code>mean_officer</code></td>
                                <td>0.09</td>
                            </tr>
                            <tr>
                                <td><code>old_wise_quirky</code></td>
                                <td>0.23</td>
                            </tr>
                            <tr>
                                <td><code>respected_leader</code></td>
                                <td>0.21</td>
                            </tr>
                            <tr>
                                <td><code>shallow_and_popular</code></td>
                                <td>0.39</td>
                            </tr>
                            <tr>
                                <td><code>sidekick</code></td>
                                <td>0.15</td>
                            </tr>
                            <tr>
                                <td><code>skilled_badass</code></td>
                                <td>0.13</td>
                            </tr>
                            <tr>
                                <td><code>tech_genius</code></td>
                                <td>0.15</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption>Table of F1-Scores per Trope</figcaption>
            </figure>
        </p>
        <br>
        <p>
            As said above, a high F1-score means that the model has a good balance at not only identifying most of the true tropes but also maintaining a low rate of false positives.
            We can observe <code>laidback_freebird</code> as a trope with seemingly distinctive features.
        </p>
        <p>
            As such, to select the tropes that are best described by our model, we set a F1-score threshold of <b>0.15</b>.
            This allows us to focus our energy on the tropes where the link between features and trope is strong and statistically significant, potentially leading to more confident insights into Hollywood's typecasting patterns.
            We select the following tropes:
            <ul>
                <li><code>charimastic_charmer</code></li>
                <li><code>jock</code></li>
                <li><code>dumb_and_clumsy</code></li>
                <li><code>emotional_damage</code></li>
                <li><code>laidback_freebird</code></li>
                <li><code>old_wise_quirky</code></li>
                <li><code>shallow_and_popular</code></li>
                <li><code>respected_leader</code></li>
            </ul>
        </p>
        <br>
        <h2>Individual Model Training</h2>
        <p>
            We now have 8 character tropes of interest.
            After a multi-class approach in the step above, I think these each tropes are craving a bit of one-on-one attention.
            Turns out there was more than just the F1-score that was important about our decision tree <i>(Shocker, right?!)</i>.
            The decision tree gives us another delicious fruit, as it evaluates the importance of each feature, defined previously in our <a href="https://tim1110mann.github.io/ADAcadabra1000101/preprocessing.html">pre-processing</a> step, for each trope.
        </p>
        <br>
        <p>
            We can visualise these results in this nice heat map:
            <figure>
                <img src="causalanalysis_plots/Heatmap.png">
                <figcaption>Heatmap of the Feature Importance per Trope</figcaption>
            </figure>
            
        </p>
        <br>
        <p>
            There's quite a bit of interesting results to pick from this heat map. 
            Note that the redder the box, the more important the feature is for that trope, the bluer, the less important it is. 
            Let's have a look at the most notable results.
        </p>
        <p>
            For the <code>shallow_and_popular</code> trope, we see that being a woman is a very important feature <i>(we set 1 as female and 0 as male)</i>.
            I wonder if the <a href="https://tim1110mann.github.io/ADAcadabra1000101/unsupervised-learning.html">unsupervised learning</a> narrator has something to say about...
            We also observe that you need to be old to play the <code>old_wise_quirky</code> trope (that was not really much a surprise, but I'd still appreciate an audible gasp of shock).
            A really interesting one is for the <code>emotional_damage</code> trope with a high value for the <code>Face Shape</code> feature. 
            Perhaps, this could suggest that actors with a rounder face shape are more likely to play the <code>emotional_damage</code> trope. 
        </p>
        <p>
            However, we notice that for some tropes, features seem to have little to no importance. 
            We're looking at you <code>respected_leader</code>...
        </p>
        <br>
        <p>
            As such, to delve deeper into our results, we evaluate the statistical significance of each feature for each trope, to make sure they actually mean something.
            We confirm our result from above about the <code>respected_leader</code> trope as the p-values for each features for this particular archetype are absurdly high.
            So for each trope, we only plotted the coefficients of their statistically significant features (so don't be surprised when some features and tropes seem to dissapear!).
            Please note that we separated the <code>ActorGender</code> variable into two, one for female and one for male. 
            <figure>
                <iframe src="causalanalysis_plots/significantcoef_plot.html" width=100% height="600"></iframe>
                <figcaption>Bar Chart of the Coeffiecients of the Statistically Significant Features of Each Trope</figcaption>
            </figure>
        </p>
        <br>
        <p>
            To wrap things up, we were able to identify which features were prevalent in certain tropes. 
            This gives weight behind our suggestions that actors with particular features are best suited to play a specific character trope.
            For example, an old actor should play the <code>old_wise_quirky</code> trope. Ok, not my best example but you get it.
        </p>
        <br>
        <p>
            I'll let you go pay a visit to my <a href="https://tim1110mann.github.io/ADAcadabra1000101/unsupervised-learning.html">unsupervised learning</a> narrator. 
            Say hi to them for me!
        </p>
        <h3><a href="https://tim1110mann.github.io/ADAcadabra1000101/unsupervised-learning.html">Unsupervised Learning</a></h3>
        <p>
            Oh, so you didn't read me first. 
            Well, can't do much about it now, can I? At least you did read my chapter, so thanks! 
            Time to hop to the endgame now.
        </p>
        <h3> <a href="https://tim1110mann.github.io/ADAcadabra1000101/conclusion.html">Conclusion</a></h3>
    </div>
</body>
</html>
